{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164c89ee",
   "metadata": {},
   "source": [
    " # Курсовая работа\n",
    " ## на тему \"Датасеты\"\n",
    "\n",
    "1) Выбор своего датасета (условие — выполнено в виде интерактивной презентации в юпитере)\n",
    "\n",
    "Нужно попытаться визуализировать экземпляр данных\n",
    "\n",
    "2) найти публикацию\\статью про датасет, внедрить ссылку и кратко описать в презентации (+ какие популярные модели используются)\n",
    "\n",
    "3) Пример данных с разметкой + пример кода для загрузки тестового набора\n",
    "\n",
    "4) Пример применения готовой модели на этих данных\n",
    "\n",
    "!) Обязательное использование Markdown + LaTeX\n",
    "\n",
    "!) Обязательное использование Plotly\n",
    "\n",
    "!) Обязательное использование готового кода с алгоритмом относительно датасета\n",
    "\n",
    "!) Обязательно воспользоваться доплнительно TensorBoard или аналогом\n",
    "\n",
    "*) (realesrgan, pifuhd) Препроцессинг другой моделью (+ описать)\n",
    "\n",
    "Титульник надо напечатать + ссылка на ноутбук"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdf227-3c6b-4503-8523-0b148a9bb607",
   "metadata": {},
   "source": [
    "## 1 + 2) Выбор Датасета, Публикация\\Статья про датасет, ссылка, краткое описание\n",
    "Мой выбор пал на датасет Reddit. Ссылка: https://paperswithcode.com/dataset/reddit\n",
    "\n",
    "Данный датасет представляет собой датасет графов, состоящий из постов сайта Reddit, сделанного в месяце сентября 2014 года. Данные разделены на сообщества или \"Суб-Реддиты\", каждому из которых принадлежит своё сообщение. Сообщения были взяты из 50 крупнейших сообществ и были построены в графике \"Публикация-к-публикации\", соединяющие посты одного пользователя. В общей сложности датасет состоит из 232 965 постов в средней степенью 492. Первые 20 дней используются для обучения, остальные дни - для тестирования (из них 30% используются для валидации). Для моделей подготовлены 300-мерные векторы слов GloVe CommonCrawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613993c-e0ba-4d55-b161-fefae4108063",
   "metadata": {},
   "source": [
    "## 3) Пример данных с разметкой + пример кода для загрузки тестового набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d43c5d64-6d7e-4b16-8899-3995b305be34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7584\\2078981986.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m from torch_geometric.data import (\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mInMemoryDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MAI_114M\\lib\\site-packages\\torch_geometric\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MAI_114M\\lib\\site-packages\\torch_geometric\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MAI_114M\\lib\\site-packages\\torch_geometric\\data\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "\n",
    "class Reddit(InMemoryDataset):\n",
    "    r\"\"\"The Reddit dataset from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper, containing\n",
    "    Reddit posts belonging to different communities.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    Stats:\n",
    "        .. list-table::\n",
    "            :widths: 10 10 10 10\n",
    "            :header-rows: 1\n",
    "\n",
    "            * - #nodes\n",
    "              - #edges\n",
    "              - #features\n",
    "              - #classes\n",
    "            * - 232,965\n",
    "              - 114,615,892\n",
    "              - 602\n",
    "              - 41\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://data.dgl.ai/dataset/reddit.zip'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['reddit_data.npz', 'reddit_graph.npz']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "    def process(self):\n",
    "        data = np.load(osp.join(self.raw_dir, 'reddit_data.npz'))\n",
    "        x = torch.from_numpy(data['feature']).to(torch.float)\n",
    "        y = torch.from_numpy(data['label']).to(torch.long)\n",
    "        split = torch.from_numpy(data['node_types'])\n",
    "\n",
    "        adj = sp.load_npz(osp.join(self.raw_dir, 'reddit_graph.npz'))\n",
    "        row = torch.from_numpy(adj.row).to(torch.long)\n",
    "        col = torch.from_numpy(adj.col).to(torch.long)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data.train_mask = split == 1\n",
    "        data.val_mask = split == 2\n",
    "        data.test_mask = split == 3\n",
    "\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac028dc7-ad5a-4866-a1b6-d4ee90f1c11d",
   "metadata": {},
   "source": [
    "## 4) Пример применения готовой модели на этих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb15b9-322e-4b1d-807c-fc545c991382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MAI_114M] *",
   "language": "python",
   "name": "conda-env-MAI_114M-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
